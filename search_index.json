[
["index.html", "R para jurimetria Capítulo 1 Apresentação", " R para jurimetria Associação Brasileira de Jurimetria 15 de January de 2018 Capítulo 1 Apresentação Olá! Bem vindo ao curso R para Jurimetria :) Essa é a primeira iteração do curso de R para Jurimetria da ABJ. Nesse curso abordamos aspectos práticos da Jurimetria, essenciais para um profissional da Estatística que tenha interesse em trabalhar nessa área. O curso é voltado para graduandos em estatística e está organizado em cinco aulas. (15/01) Introdução à jurimetria e setup Definição Terminologia Tipos de pesquisa e exemplos Ambientação com R e RStudio RMarkdown (17/01) Ferramentas da ABJ para importação Introdução ao web scraping com httr e rvest. Pacotes abjutils, esaj e dje. (19/01) Tidyverse: vetores Vetores: pacotes stringr, lubridate, forcats. Programação funcional: purrr. (22/01) Tidyverse: bd Transformação: pacotes dplyr, tidyr. Aplicação: Câmaras criminais. (24/01) Modelagem Especialização de varas: pacotes bnlearn e survival. Captchas: pacotes keras e decryptr. Para o curso, será necessário ter conhecimentos do software estatístico R, como a lógica de programação, sintaxe do R e ambientação com o RStudio. "],
["introducao.html", "Capítulo 2 Introdução", " Capítulo 2 Introdução Bem vinda ao maravilhoso mundo da jurimetria. Essa é sua última chance. Está preparada? "],
["o-que-e-jurimetria.html", "2.1 O que é Jurimetria?", " 2.1 O que é Jurimetria? A jurimetria é o estudo empírico do direito. Ela se distingue das demais disciplinas jurídicas por tratar o direito de forma concreta. A pesquisa em jurimetria utiliza dados do judiciário para avaliar desde argumentos quantitativos a serem utilizados por juristas e advogados até o impacto de leis. No Brasil, é utilizada principalmente como ferramenta para auxiliar na formulação de políticas públicas e melhorar a administração dos tribunais. A jurimetria está para o Direito da mesma forma que a econometria, a biometria e a sociometria estão, respectivamente, para a Economia, Biologia e Sociologia. O Direito, embora conte com algumas áreas tradicionalmente mais empíricas que outras, ainda não tem uma disciplina mais formalizada. Nesse sentido, uma das missões da Associação Brasileira de Jurimetria (ABJ) é incentivar e divulgar a jurimetria aos seus associados e ao público em geral. Nossos estudos e pesquisas visam a difusão e o desenvolvimento do campo no Brasil, agregando pesquisadores e fornecendo o ferramental necessário para realização das análises. Neste curso, apresentaremos o ferramental de trabalho desenvolvido no laboratório da ABJ. As ferramentas servem principalmente para extração e arrumação de dados, deixando o caminho livre para realização de análises estatísticas mais aprofundadas. "],
["tipos-de-estudo.html", "2.2 Tipos de estudo", " 2.2 Tipos de estudo No decorrer do curso trabalharemos dois tipos comuns de estudos jurimétricos: prospectivo e retrospectivo. É importante separar esses tipos de estudo tanto pela matéria desenvolvida quanto pelas fontes de dados disponíveis. Faremos isso através de um exemplo. 2.2.1 Exemplo Digamos que a NET tenha colocado seu nome indevidamente no Serasa e você processa ela. Quando você entra com um processo, ele é distribuído (levado) numa vara (casinha do juiz), por exemplo uma vara cível de São Carlos, e seu processo estará na 1a instância. O juiz então irá decidir sobre o caso, produzindo uma sentença (um texto de 5-10 páginas que explica o que ele decidiu e por quê). Se você ou a NET ficarem infelizes com o resultado, vocês podem entrar com um recurso de apelação (evolução de processo), que será distribuído numa câmara (evolução de vara) do Tribunal de Justiça de São Paulo (TJSP, que fica na praça da Sé, em São Paulo), a ser julgada por desembargadores (evolução de juiz), e seu processo estará na 2a instância. Os desembargadores então irão decidir sobre o caso, produzindo um acórdão (evolução de sentença). Se você ou a NET ficarem infelizes com o resultado, vocês podem novamente entrar com recursos, que irão para o STJ e posteriormente ao STF (em Brasília), a serem julgados por Ministros (evolução de desembargador). Nossos estudos restringem-se ao TJSP, envolvendo (1) e (2). O TJSP atualmente é o mais simples para obtenção automática de processos. 2.2.2 Divisão regional O TJSP é dividido em comarcas, circunscrições e regiões administrativas. As comarcas são conjuntos de um ou mais municípios e uma comarca deve sempre ter uma ou mais varas. Circunscrições e regiões administrativas existem somente para 2.2.3 Qual tipo escolher? Estudo prospectivo é o estudo que acompanha o processo judicial desde a data de distribuição até o fim. O fim pode ser marcado pela data da sentença, acórdão, ou outro evento de interesse. Ou seja, os casos são indexados pela data de nascimento, e acompanhados até a data de sua morte. Em muitos casos, os processos ainda não atingiram o fim no momento da realização do estudo. Estudo retrospectivo é o estudo que levanta processos que acabaram (por sentença ou por acórdão) e analisa suas características. Ou seja, os casos são indexados pela data de morte. A Figura abaixo mostra os diferentes escopos. ( 1) Prospectivo e retrospectivo. ( 2) Apenas prospectivo. ( 3) Apenas retrospectivo. ( 4) Nenhum dos dois, mas poderia ser capturado por atividade no período. (5 e 6) fora do escopo. ( 7) Nenhum dos dois tipos e não poderia ser capturado (ficou inativo no período). Estudos prospectivos são úteis quando o intuito é estudar o tempo das fases do processo. Já estudos retrospectivos são úteis para a análise do perfil de decisões. Estudos que analisam tempos em bases retrospectivas. Se quiser saber mais sobre isso, veja esse texto. "],
["estudo-1-camaras-criminais.html", "2.3 Estudo 1: Câmaras criminais", " 2.3 Estudo 1: Câmaras criminais O Direito Criminal é uma área que traz consigo diversas questões difíceis e importantes da nossa sociedade. Uma destas questões, que remete ao possível descolamento da teoria do Direito e o que ocorre no mundo real, trata do cumprimento da pena. Considerando-se o plano ideal e o princípio de ampla defesa, mas também a conhecida morosidade dos tribunais, qual é o momento do processo em que deveria ser iniciado o cumprimento de pena? Será que a taxa de reforma das decisões é tão pequena a ponto de justificar o início do cumprimento de pena após a sentença na primeira instância? Com o objetivo de obter essas taxas, a presente pesquisa utiliza como base de dados um levantamento de 157.379 decisões em segunda instância, das quais 57.625 envolvem apelações contra o Ministério Público, todas proferidas entre 01/01/2014 e 31/12/2014 nas dezesseis Câmaras de Direito Criminal do Estado de São Paulo, e nas quatro Câmaras Extraordinárias. A Figura 2.1 mostra a distribuição dos resultados dos processos em relação aos órgãos julgadores. Aqui, encontramos discrepâncias enormes, onde podemos encontrar câmaras com mais de 75% de recursos negados (quarta e sexta) e câmaras com menos de 30% de recursos negados (primeira, segunda e décima segunda). Este resultado poderia ser explicado por duas hipóteses: os processos não são distribuídos aleatoriamente nas câmaras, e é feita uma triagem que envolve o tipo do recurso; ou os magistrados de cada câmara comportam-se de maneiras muito diferentes, mesmo para processos considerados homogêneos. Figura 2.1: Resultados por câmara. "],
["estudo-2-especializacao-de-varas-empresariais.html", "2.4 Estudo 2: Especialização de varas empresariais", " 2.4 Estudo 2: Especialização de varas empresariais 2.4.1 Contextualização Apesar do município de São Paulo ser o maior polo empresarial do Brasil, a comarca de São Paulo não possuía varas especializadas nessa matéria. Por outro lado, o provimento nº 82/2011 do TJSP define critérios para criação de varas: Uma vara só pode ser criada se o volume de trabalho esperado for maior do que 1.800 processos/ano. A análise deve levar em conta a complexidade dos feitos. Em levantamentos anteriores, foram observadas menos de 1.800 distribuições de processos em um ano e, portanto, não seria possível justificar a criação dessas varas. No entanto, é sabido que processos empresariais, em média, são mais complexos que processos cíveis. Por isso, faria sentido criar varas empresariais, ainda que a meta quantitativa não fosse atingida. Como verificar essa hipótese? Nesse estudo, buscamos formas de comparar a complexidade processos cíveis e empresariais. 2.4.2 Desafio adicional: cifra oculta Em estudos jurimétricos, até mesmo a pergunta “quantos processos empresariais são distribuídos por ano na comarca de São Paulo?” é complicada. Os registros são imprecisos e a classificação proposta pelo CNJ não é bem utilizada. Atualmente, a forma mais direta de identificar tipos de processos judiciais é utilizando os chamados assuntos processuais. Os assuntos relacionam-se com as matérias discutidas em cada caso. Por exemplo, um caso cível de indenização por dano moral poderia ter um assunto “Indenização por dano moral”, enquanto um processo falimentar de uma empresa em Recuperação Judicial poderia ser classificado como “Convocação de Recuperação Judicial em Falência”. Nesse contexto, um importante passo foi dado com a Resolução 46/2007 do CNJ1, que criou as TPUs. As TPUs são uma documentação oficial de todas as classes, assuntos e movimentações dos processos. As TPUs foram implantadas em todas as Justiças, o que facilita a realização de análises que comparam diferentes tribunais. A Figura ?? mostra uma parte das TPUs do CNJ. As TPUs são estruturadas em formato de árvore. Isso significa que temos assuntos genéricos e assuntos específicos, sendo que o assunto específico é um filho do assunto genérico. As TPUs podem ter até seis níveis hierárquicos de assuntos. O problema enfrentado atualmente é que, na prática, nem sempre os processos são classificados com assuntos específicos. Assim, podemos ter um caso que discute sobre “Análise de Crédito” classificado como “Responsabilidade do Fornecedor”, ou ainda “Direito do Consumidor”. A existência de casos classificados com assuntos genéricos implica num problema para o levantamento do volume processual por assunto. Por exemplo, considere que há interesse em conhecer o volume de processos envolvendo “Análise de Crédito”. Se considerarmos somente os casos classificados corretamente, estaríamos subestimando o real volume de processos, pois estaríamos ignorando os casos classificados em assuntos genéricos. Por outro lado, se considerarmos no levantamento todos os casos, incluindo os genéricos, estaríamos superestimanto o real volume. A possibilidade de subestimação do volume real de processos de um certo tipo configura o que chamamos de cifra oculta. Dado um assunto específico, esse número pode ser definido como a quantidade de processos com esse assunto, mas classificados em assuntos genéricos. Felizmente, a cifra oculta pode ser estimada. Para isso, no entanto, é necessário fazer algumas suposições ou utilizar conhecimentos a priori sobre os processos. A forma mais simples de estimar a cifra oculta é realizando uma espécie de regra de três. No exemplo da análise de crédito, considere que temos uma base de dados com todos os casos classificados com assuntos dentro da árvore do “Direito do Consumidor”. Suponha também que todos os processos de análise de crédito foram classificados ou corretamente, ou incorretamente como “Direito do Consumidor”. Utilizando somente a parte da base que foi classificada com assuntos específicos, estimamos a proporção de casos \\(p\\) classificados como “Análise de Crédito”. Assim, uma estimativa do volume de processos de análise de crédito é dada por \\[ N_{\\text{cred}}=N_A+N_T\\times p, \\text{onde} \\] \\(N_A\\) é o volume de casos classificados corretamente como “Análise de Crédito”. \\(N_T\\) é o volume total de casos classificados como “Direito do Consumidor”. \\(N_T\\times p\\) é a estimativa da cifra oculta. Na nossa aplicação, isso foi feito estimando-se a probabilidade de um assunto genérico tratar da matéria empresarial, usando-se uma rede bayesiana. Para esse cálculo, utilizamos a parcela da base de dados que foi classificada corretamente e calculamos a proporção de processos empresariais para cada assunto. A cifra oculta é estimada somando-se as probabilidades obtidas. Nossos dados originais são: No Foro Central Cível foram distribuídos 675 processos empresariais por ano. Nos demais Foros foram distribuídos 450 processos por ano. Total: 1.125 processos/ano. Aplicando as correções, temos: No Foro Central Cível foram distribuídos 961 processos empresariais/ano. Nos demais Foros foram distribuídos 641 processos empresariais/ano. Total: 1.602 processos/ano. 2.4.3 Voltando à complexidade Agora vamos fazer comparações. O processo demora mais tempo como um todo? \\[ \\text{Complexidade} = \\text{Tempo entre distribuicao e sentenca} \\] O processo custa mais tempo aos magistrados? \\[ \\text{Complexidade} = \\text{Tempo entre conclusos e decisoes} \\] Nas duas métricas, processos empresariais são mais complexos. Comparando casos de dificuldade moderada, processos empresariais custam o dobro de tempo para magistrados e 30% a mais de tempo na tramitação total. Aplicando os critérios propostos, a carga de trabalho proporcionada pelos processos empresariais corresponde a - 2.082 processos comuns, considerando o custo em tramitação; - 3.349 processos comuns, considerando o tempo gasto pelos magistrados. http://www.cnj.jus.br/images/stories/docs_cnj/resolucao/rescnj_46.pdf. Acesso em 26/06/2017.↩ "],
["exemplo-3-captchas.html", "2.5 Exemplo 3: CAPTCHAs", " 2.5 Exemplo 3: CAPTCHAs Apesar dos sistemas jurídicos serem públicos, seus dados não são acessíveis. Muitas vezes o acesso às páginas web são limitadas através de bloqueios de IP e Algumas justificativas para a existência de CAPTCHAs são: i) não onerar os sistemas ou ii) a ideia de que assim estão protegendo as pessoas. O primeiro argumento é frágil pois ambos poderiam ser resolvidos através de uma API de acesso público dos tribunais, ou mesmo uma API paga. O segundo argumento também é ruim, pois, ainda que limitado, esses dados serão obtidos e utilizados. Limitar o acesso só aumenta o custo para construção dessas bases, direcionando o poder para as empresas que têm mais dinheiro para investir nisso, causando viés no acesso à informação. Fazendo curta uma história longa, se um dado é público, ele deve ser acessível. Nos trabalhos da ABJ, esbarramos com CAPTCHAs inúmeras vezes. Recentemente, descobrimos formas de resolver CAPTCHAs automaticamente utilizando modelos estatísticos. Esses modelos são da recente (ou não) classe de modelos de deep learning, uma área que cresceu exponencialmente nos últimos anos. Incluímos esse exemplo no curso por dois motivos. Apesar de não ser um modelo jurimétrico, trata-se de um problema presente no contexto da jurimetria, logo é um conhecimento útil. Além disso, a técnica utilizada para quebrar CAPTCHAs é muito interessante e poderia ser adaptada para diversos contextos, inclusive estudos jurimétricos. Nossas soluções para quebrar captchas foram consolidadas num pacote chamado decryptr. Vamos discutir brevemente como ele foi criado e como usar. Também discutiremos superficialmente o modelo de redes neurais utilizado e como criar o seu próprio modelo. Veja um quebrador de captcha em funcionamento: "],
["cuidado.html", "2.6 Cuidado", " 2.6 Cuidado As bases de dados utilizadas em estudos jurimétricos foram originalmente concebidas para fins gerenciais e não analíticos. Por isso, observamos muitos dados faltantes, mal formatados e com documentação inadequada. Uma boa porção dos dados só está disponível em páginas HTML e arquivos PDF e grande parte da informação útil está escondida em textos. Chamamos esse fenômeno de “pré-sal sociológico”. Temos hoje diversas bases de dados armazenadas em repositórios públicos ou controladas pelo poder público, mas que precisam ser lapidadas para obtenção de informação útil. O jurimetrista trabalha com dados sujos e desorganizados, mas gera muito valor ao extrair suas informações. Por isso, o profissional precisa dominar o ferramental de extração, transformação e visualização de dados, e é sobre isso que discutiremos na primeira parte do curso. Utilizaremos como base o software estatístico R, que atualmente possui diversas ferramentas que ajudam nessas atividades. "],
["organizacao-do-curso.html", "2.7 Organização do curso", " 2.7 Organização do curso O curso foi montado em RMarkdown, usando um pacote chamado bookdown. O material é 100% reprodutível e está disseminado em diversos lugares no GitHub. As aulas foram organizadas pelo Ciclo de ciência de dados. Trabalharemos com o tidyverse Import: esaj e dje. Tidy / Transform: abjutils, dplyr, stringr, lubridate, forcats, tidyr e purrr. Visualize: ggplot2. Model: survival, bnlearn, keras "],
["configuracao-necessaria-para-o-curso.html", "2.8 Configuração necessária para o curso", " 2.8 Configuração necessária para o curso 2.8.1 Pacotes Para poder rodar os códigos do curso, é importante que você instale todos os pacotes relacionados a este livro. Para isso, basta rodar devtools::install_github(&quot;abjur/r4jurimetrics&quot;) Esse código teoricamente funciona pois este livro também é um pacote e adicionamos todas as dependências. Se você não conseguir instalar as dependências, abra o arquivo DESCRIPTION do material: https://github.com/abjur/r4jurimetrics/blob/master/DESCRIPTION Nesse aquivo constam todos os pacotes que estamos utilizando. Se algo deu errado na instalação do r4jurimetrics, é porquê pelo menos um desses pacotes deu erro na instalação. Note que os pacotes dentro de Remotes devem ser instalados com devtools::install_github(). Note também que os pacotes que estão no Remotes também estão no Imports. 2.8.2 Fork e upstream (experimental) Para fazer essa parte funcionar, você precisará de uma conta ativa no GitHub. Se você não tiver uma, recomendamos fortemente que criem. Agora siga os passos abaixo: Acesse no seu navegador: https://github.com/abjur/r4jurimetrics Certifique-se de que você está logado na sua conta. Dê um fork no repositório Volte ao RStudio Crie um projeto: Projetos &gt; New Project &gt; Version control &gt; git Na parte do link do repositório, coloque https://github.com//r4jurimetrics Dê OK e certifique-se de que o repositório foi criado. Abra um terminal: Tools &gt; Shell Arrume e rode git config --global user.email &quot;seuEmail&quot; e git config --global user.name &quot;seuLogin&quot; Esses comandos configuram seu usuário para registrar mudanças nos repositórios. Rode git remote add upstream https://github.com/abjur/r4jurimetrics.git Esse comando faz com que seu repositório fique ligado ao repositório original Rode git fetch upstream Esse comando baixa as atualizações do repositório pai, se houverem Rode git merge upstream/master -m &quot;merge with parent&quot; Esse comando juntar o seu repositório com o código do reporitório pai. Nesse momento podem acontecer conflitos. Se tiver algum problema, veja: https://help.github.com/articles/configuring-a-remote-for-a-fork/ https://help.github.com/articles/syncing-a-fork/ "],
["suas-tarefas-para-1701-demora-30-min.html", "2.9 Suas tarefas para 17/01 (demora 30 min)", " 2.9 Suas tarefas para 17/01 (demora 30 min) Entrar no grupo: t.me/rbrasil Cadastrar na newsletter da ABJ http://www.abj.org.br Ler blog: http://www.abj.org.br/blog/2017/01/27/2017-01-28-assuntos/ Ler blog: http://www.abj.org.br/blog/2016/12/31/2016-12-31-tempos/ Dar uma olhada em https://abjur.github.io/tjspBook Dar uma olhada em http://material.curso-r.com Dar uma olhada em https://tidyverse.org Dar uma olhada em http://r4ds.had.co.nz Dar uma olhada em https://github.com/abjur, https://github.com/courtsbr, https://github.com/curso-r "],
["importacao-de-dados.html", "Capítulo 3 Importação de dados ", " Capítulo 3 Importação de dados "],
["pacotes-httr-xml2-e-rvest.html", "3.1 Pacotes httr, xml2 e rvest", " 3.1 Pacotes httr, xml2 e rvest Esses são os três pacotes mais modernos do R para fazer web scraping. O pacote xml2 tem a finalidade de estruturar arquivos HTML ou XML de forma eficiente, tornando possível a obtenção de tags e seus atributos dentro de um arquivo. Já o pacote httr é responsável por realizar requisições web para obtenção das páginas de interesse, buscando reduzir ao máximo a complexidade da programação. O pacote rvest é escrito sobre os dois anteriores e por isso eleva ainda mais o nível de especialização para raspagem de dados. As características dos pacotes implicam na seguinte regra de bolso. Para trabalhar com páginas simples, basta carregar o rvest e utilizar suas funcionalidades. Caso o acesso à página exija ações mais complexas e/ou artifícios de ferramentas web, será necessário utilizar o httr. O xml2 só será usado explicitamente nos casos raros em que a página está em XML, que pode ser visto como uma generalização do HTML. Esses pacotes não são suficientes para acessar todo tipo de conteúdo da web. Um exemplo claro disso são páginas em que o conteúdo é produzido por javascript, o que acontece em alguns sites modernos. Para trabalhar com esses sites, é necessário realmente “simular” um navegador que acessa a página web. Uma das melhores ferramentas para isso é o selenium. Não discutiremos selenium nesse curso, mas caso queira se aprofundar, acesse aqui e o pacote RSelenium. 3.1.1 Sessões e cookies No momento que acessamos uma página web, nosso navegador baixa alguns arquivos que “identificam” nosso acesso à página. Esses arquivos são chamados cookies e são usados pelos sites para realizar diversas atividades, como carregar uma página pré-definida pelo usuário caso este acesse o site pela segunda vez. O httr e por consequência o rvest já guardam esses cookies de forma automática, de forma que o usuário não precise se preocupar com isso. Em casos raros, para construir o web scraper é necessário modificar esses cookies. Nesses casos, estude a função cookies() do httr. 3.1.2 GET e POST Uma requisição GET envia uma url ao servidor, possivelmente com alguns parâmetros nessa url (que ficam no final da url depois do ?). O servidor, por sua vez, recebe essa url, processa os parâmetros e retorna uma página HTML para o navegador2. A requisição POST, no entanto, envia uma url não modificada para o servidor, mas envia também uma lista de dados preenchidos pelo usuário, que podem ser números, textos ou até imagens. Na maioria dos casos, ao submeter um formulário de um site, fazemos uma requisição POST. O httr possui os métodos GET e POST implementados e são muito similares. A lista de parâmetros enviados pelo usuário pode ser armazenado numa list nomeada, e adicionado ao GET pelo parâmetro query ou no POST pelo parâmetro body. Veremos exemplos disso mais adiante. 3.1.3 Outras funções do httr Outras funções úteis: write_disk() para escrever uma requisição direto em disco, além de guardar na memória RAM. config() para adicionar configurações adicionais. Por exemplo, quando acessar uma página https com certificados inadequados numa requisição GET, rode GET('https://www...', config(ssl_verifypeer=F)). oauth_app() para trabalhar com APIs. Não discutiremos conexão com APIs nesse curso, mas é um importante conceito a ser estudado. 3.1.4 Principais funções do rvest Para acessar páginas da web: html_session() abre uma sessão do usuário (baixa página, carrega cookies etc). follow_link(), jump_to() acessa uma página web a partir de um link (tag &lt;a&gt;) ou url. html_form() carrega todos os formulários contidos numa página. set_value() atribui valores a parâmetros do formulário. submit_form() submete um formulário obtido em html_form. Para trabalhar com arquivos HTML: read_html() lê o arquivo HTML de forma estruturada e facilita impressão. html_nodes() cria uma lista com os nós identificados por uma busca em CSS path ou XPath. html_node() é um caso especial que assume que só será encontrado um resultado. html_text() extrai todo o conteúdo de um objeto e retorna um texto. html_table() extrai o conteúdo de uma &lt;table&gt; e transforma em um data_frame. html_attr() extrai um atributo de uma tag, por exemplo href da tag &lt;a&gt;. 3.1.5 CSS path e XPath O CSS path e o XPath são formas distintas de buscar tags dentro de um documento HTML. O CSS path é mais simples de implementar e tem uma sintaxe menos verborrágica, mas o XPath é mais poderoso. A regra de bolso é tentar fazer a seleção primeiro em CSS e, caso não seja possível, implementar em XPath. Esses paths serão mostrados en passant durante o curso, mas não serão abordados em detalhe. Caso queira se aprofundar no assunto, comece pela ajuda da função ?html_nodes. 3.1.6 APIs com httr O httr foi criado pensando-se nas modernas APIs que vêm sendo desenvolvidas nos últimos anos. O httr já tem métodos apropriados para trabalhar com Facebook, Twitter e Google, entre outros. Para um guia completo de como utilizar APIs no R, acesse esse tutorial. Um exemplo de pacote que utiliza API usando esse tutorial melhores práticas pode ser acessado aqui. para entender sobre server side e user side, acesse server side e user side.↩ "],
["web-scraping.html", "3.2 Web scraping", " 3.2 Web scraping Esta seção contém algumas melhores práticas na contrução de ferramentas no R que baixam e processam informações de sites disponíveis na web. O objetivo é ajudar o jurimetrista a desenvolver programas que sejam fáceis de adaptar no tempo. É importante ressaltar que só estamos trabalhando com páginas que são acessíveis publicamente. Caso tenha interesse e “raspar” páginas que precisam de autenticação, recomendamos que estude os termos de uso do site. Para ilustrar este texto, usaremos como exemplo o código utilizado no trabalho das câmaras, que acessa o site do Tribunal de Justiça de São Paulo para obter informações de processos judiciais. Trabalharemos principalmente com a Consulta de Jurisprudência e a Consulta de de Processos de Segundo Grau do TJSP. 3.2.1 Informações iniciais Antes de iniciar um programa de web scraping, verifique se existe alguma forma mais fácil de conseguir os dados que necessita. Construir um web scraper do zero é muitas vezes uma tarefa dolorosa e, caso o site seja atualizado, pode ser que boa parte do trabalho seja inútil. Se os dados precisarem ser extraídos apenas uma vez, verifique com os responsáveis pela manutenção do site se eles podem fazer a extração que precisa. Se os dados precisarem ser atualizados, verifique se a entidade não possui uma API para acesso aos dados. Ao escrever um web scraper, as primeiras coisas que devemos pensar são Como o site a ser acessado foi contruído, se tem limites de requisições, utilização de cookies, states, etc. Como e com que frequência o site é atualizado, tanto em relação à sua interface como em relação aos dados que queremos extrair. Como conseguir a lista das páginas que queremos acessar. Qual o caminho percorrido para acessar uma página específica. Sugerimos como melhores práticas dividir todas as atividades em três tarefas principais: i) buscar; ii) coletar e iii) processar. Quando já sabemos de antemão quais são as URLs que vamos acessar, a etapa de busca é desnecessária. Na maior parte dos casos, deixar os algoritmos de coleta e processamento dos dados em funções distintas é uma boa prática pois aumenta o controle sobre o que as ferramentas estão fazendo, facilita o debug e a atualização. Por outro lado, em alguns casos isso pode tornar o código mais ineficiente e os arquivos obtidos podem ficar pesados. 3.2.2 Diferença entre buscar, baixar e processar. Buscar documentos significa, de uma forma geral, utilizar ferramentas de busca (ou acessar links de um site) para obter informações de uma nova requisição a ser realizada. Ou seja, essa etapa do scraper serve para “procurar links” que não sabíamos que existiam previamente. Isso será resolvido através da função esaj::download_cjsg(). Baixar documentos, no entando, significa simplesmente acessar páginas pré-estabelecidas e salvá-las em disco. Em algumas situações, os documentos baixados (depois de limpos) podem conter uma nova lista de páginas a serem baixadas, formando iterações de coletas. A tarefa de baixar documentos pré-estabelecidos será realizada pelas funções esaj::download_cposg() e dje::download_dje(). Finalmente, processar documentos significa carregar dados acessíveis em disco e transformar os dados brutos uma base tidy. Usualmente separamos a estruturação em duas etapas: i) transformar arquivos não-estruturados em um arquivos semi-estruturados (e.g. um arquivo HTML em uma tabela mais um conjunto de textos livres) e ii) transformar arquivos semi-estruturados em uma base analítica (estruturada). A tarefa de processar as páginas HTML será realizada pelas funções esaj::parse_cjsg() e esaj::run_parser(). Na pesquisa das câmaras, o fluxo é buscar -&gt; coletar -&gt; processar -&gt; coletar -&gt; processar Na pesquisa da especialização, o fluxo é coletar -&gt; processar -&gt; coletar -&gt; processar "],
["baixando-dados-do-tjsp.html", "3.3 Baixando dados do TJSP", " 3.3 Baixando dados do TJSP Dependendo do tipo de estudo, a fonte dos dados muda. Se o estudo é retrospectivo, listamos os processos em pesquisas de julgados Se o estudo é prospectivo, listamos os processos Em ambos os casos, é recomendável que você tente obter os dados a partir de contato com os Tribunais, antes de baixar qualquer coisa da web. "],
["pacote-esaj.html", "3.4 Pacote esaj", " 3.4 Pacote esaj Onde guardar os dados? Ao construir um scraper, é importante guardar os dados brutos na máquina ou num servidor, para reprodutibilidade e manutenção do scraper. Se estiver construindo um pacote do R, o melhor lugar para guardar esses dados é na pasta data-raw, como sugerido no livro r-pkgs. Se os dados forem muito volumosos, pode ser necessário colocar esses documentos numa pasta externa ao pacote. Para garantir a reprodutibilidade, recomendamos a criação de um pacote no R cujo objetivo é somente baixar e processar esses dados, além da criação de um repositório na nuvem (Dropbox, por exemplo). No pacote que contém as funções de extração, guarde os dados já processados (se couberem) num arquivo .rda dentro da pasta data do pacote. 3.4.1 Download esaj esaj::download_cjsg(&quot;homicídio&quot;, &quot;data-raw/cjsg&quot;, max_page = 2) 3.4.2 Parse esaj files &lt;- dir(&quot;data-raw/cjsg&quot;, full.names = TRUE, pattern = &quot;page&quot;) d_cjsg &lt;- esaj::parse_cjsg(files) d_cjsg 3.4.3 Download CPOSG processos &lt;- unique(d_cjsg$id_lawsuit) esaj::download_cposg(processos, &quot;data-raw/cposg&quot;) 3.4.4 Parse CPOSG files_cposg &lt;- dir(&quot;data-raw/cposg&quot;, full.names = TRUE) parser &lt;- esaj::make_parser() %&gt;% esaj::parse_data() %&gt;% esaj::parse_parts() %&gt;% esaj::parse_movs() %&gt;% esaj::parse_decisions() d_cposg &lt;- esaj::run_parser(files_cposg, parser, &quot;data-raw/cposg_rds&quot;) 3.4.5 Download docs decisoes &lt;- unique(d_cjsg$id_decision) downloaded &lt;- tools::file_path_sans_ext(dir(&quot;data-raw/decisions&quot;, full.names = FALSE)) esaj::download_decision(setdiff(decisoes, downloaded), &quot;data-raw/decisions&quot;) "],
["pacote-dje.html", "3.5 Pacote dje", " 3.5 Pacote dje 3.5.1 Download DJE dje::download_dje(&quot;TJSP&quot;, dates = &quot;2018-01-12&quot;, path = &quot;data-raw/dje&quot;) 3.5.2 Parse DJE dje::dje_to_text(&quot;data-raw/dje/tjsp_dje_2018-01-12&quot;) dje::find_index() "],
["pacote-abjutils.html", "3.6 Pacote abjutils", " 3.6 Pacote abjutils "],
["work-in-progress.html", "Capítulo 4 Work in progress", " Capítulo 4 Work in progress "],
["work-in-progress-1.html", "Capítulo 5 Work in progress", " Capítulo 5 Work in progress "],
["work-in-progress-2.html", "Capítulo 6 Work in progress", " Capítulo 6 Work in progress "],
["references.html", "References", " References "]
]
